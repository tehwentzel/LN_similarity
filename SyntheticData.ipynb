{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from Utils import *\n",
    "\n",
    "#for getting the fisher exact test\n",
    "import rpy2.robjects.numpy2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "\n",
    "\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "from sklearn.base import ClusterMixin, BaseEstimator\n",
    "from scipy.spatial.distance import pdist, squareform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LNDataset():\n",
    "    \n",
    "    data_columns = ['Dummy ID', \n",
    "               'Affected Lymph node UPPER',\n",
    "               'Feeding tube 6m', \n",
    "               'Aspiration rate(Y/N)',\n",
    "                'Aspiration rate Pre-therapy',\n",
    "                'Aspiration rate Post-therapy',\n",
    "                'Neck boost (Y/N)',\n",
    "               'Gender',\n",
    "               'Tm Laterality (R/L)']\n",
    "    ambiguous_nodes = set(['2/3','3/4','2/3/4','/3','2/','-R4'])\n",
    "    ln_features = ['id','nodes','position','gender','scores', 'similarity',\n",
    "               'Aspiration_rate_Post-therapy','Aspiration_rate_Pre-therapy',\n",
    "               'Feeding_tube_6m','Neck_boost']\n",
    "    js_name_dict = {'Gender': 'gender', \n",
    "                 'Tm Laterality (R/L)': 'position', \n",
    "                'Feeding tube 6m': \"Feeding_tube_6m\",\n",
    "                'Aspiration rate Pre-therapy': 'Aspiration_rate_Pre-therapy',\n",
    "                'Aspiration rate Post-therapy': 'Aspiration_rate_Post-therapy',\n",
    "                'Neck boost (Y/N)': 'Neck_boost'}\n",
    "    \n",
    "    def __init__(self,\n",
    "                 cohort_data = './Anonymized_644.Updated_cleaned_v1.3.2.tsv', \n",
    "                 adjacency_path = './connectivity_646.csv'):\n",
    "\n",
    "        self.adjacency = pd.read_csv(adjacency_path, index_col = 0)\n",
    "        self.node_list = sorted(self.adjacency.columns)\n",
    "        \n",
    "        self.get_data(cohort_data)\n",
    "        \n",
    "        self.setup_grams()\n",
    "        \n",
    "    def get_data(self, cohort_data):\n",
    "        if isinstance(cohort_data, pd.DataFrame):\n",
    "            self.data = cohort_data\n",
    "            self.data.index.rename('id',inplace = True)\n",
    "            return\n",
    "        try:\n",
    "            self.data = pd.read_csv(cohort_data, engine='python',\n",
    "                       sep = '\\t',\n",
    "                       index_col=0, \n",
    "                       usecols = LNDataset.data_columns,\n",
    "                       dtype = {'Affected Lymph node UPPER': str})\n",
    "        except:\n",
    "            self.data = pd.read_csv(cohort_data, engine='python',\n",
    "                       sep = ',',\n",
    "                       index_col=0, \n",
    "                       usecols = LNDataset.data_columns,\n",
    "                       dtype = {'Affected Lymph node UPPER': str})\n",
    "        self.data = self.data.dropna(subset=['Affected Lymph node UPPER'])\n",
    "        self.data.index.rename('id',inplace = True)\n",
    "        \n",
    "    def setup_grams(self):\n",
    "        #extract all the node names and such\n",
    "        self.left_nodes = ['L'+n for n in self.adjacency.columns]\n",
    "        self.right_nodes = ['R'+n for n in self.adjacency.columns]\n",
    "        self.rpln = ['RRPLN', 'LRPLN']\n",
    "        self.nodes = self.left_nodes + self.right_nodes\n",
    "        self.all_nodes = set(self.nodes)\n",
    "        self.node_to_index = {word: position for position, word in enumerate(self.nodes)}\n",
    "        \n",
    "        self.clean_ln_data()\n",
    "        self.index = self.data.index\n",
    "        self.ids = self.index.values\n",
    "        self.monograms = self.get_monograms(self.data)\n",
    "        self.dual_bigrams = self.setup_bigrams()\n",
    "        self.dual_monograms = self.get_dual_monograms()\n",
    "        self.spatial = lambda : pd.concat([self.dual_bigrams, self.dual_monograms], axis = 1)\n",
    "        self.nonspatial = lambda : self.dual_monograms.values\n",
    "        \n",
    "    def clean_names(self,x):\n",
    "        return [re.sub('^[LR]\\s*','',x) for x in x.columns]\n",
    "        \n",
    "    def clean_names_string(self, x):\n",
    "        return ''.join(self.clean_names(x))\n",
    "        \n",
    "    def get_dual_monograms(self):\n",
    "        lgrams = self.monograms.loc[:,self.left_nodes]\n",
    "        rgrams = self.monograms.loc[:,self.right_nodes]\n",
    "        lnames = self.clean_names_string(lgrams)\n",
    "        rnames = self.clean_names_string(rgrams)\n",
    "        assert(lnames == rnames)\n",
    "        df_values = lgrams.values + rgrams.values\n",
    "        dual_monograms = pd.DataFrame(df_values, columns = self.clean_names(lgrams), index = lgrams.index)\n",
    "        return dual_monograms\n",
    "        \n",
    "    def setup_bigrams(self):\n",
    "        l_bigrams = self.bigramize(self.monograms.loc[:, self.left_nodes], 'L')\n",
    "        r_bigrams = self.bigramize(self.monograms.loc[:, self.right_nodes], 'R')\n",
    "        rnames = self.clean_names_string(l_bigrams)\n",
    "        lnames = self.clean_names_string(r_bigrams)\n",
    "        assert(rnames == lnames)\n",
    "        assert(np.all(l_bigrams.index == r_bigrams.index))\n",
    "        dual_bigrams = pd.DataFrame(l_bigrams.values +r_bigrams.values, \n",
    "                               columns = self.clean_names(l_bigrams),\n",
    "                               index = self.index)\n",
    "        return dual_bigrams\n",
    "        \n",
    "    #helper functions\n",
    "    def parse_lymph_nodes(self, node_string):\n",
    "        #the data apparently has just '2' when theres a '2A' and '2B'\n",
    "        node_string = re.sub('L2,*','L2A, L2B,', node_string)\n",
    "        node_string = re.sub('R2,*','R2A, R2B,', node_string)\n",
    "        node_string = re.sub('R RPLN', 'RRPLN', node_string)\n",
    "        node_string = re.sub('L RPLN', 'LRPLN', node_string)\n",
    "        nodes = [n.strip() for n in node_string.split(',')]\n",
    "        #remove the node with 'in-between' labeled nodes?\n",
    "        for n in nodes:\n",
    "            if n in LNDataset.ambiguous_nodes:\n",
    "                return np.NaN\n",
    "        nodes = [n for n in nodes if n in self.all_nodes]\n",
    "        return nodes if len(nodes) > 0 else np.NaN\n",
    "    \n",
    "    def clean_ln_data(self):\n",
    "        self.data['nodes'] = (self.data.copy())['Affected Lymph node UPPER'].apply(self.parse_lymph_nodes).values\n",
    "        self.data = self.data.dropna(subset=['nodes'])\n",
    "\n",
    "    def get_monograms(self, data):\n",
    "        monograms = pd.DataFrame(index = self.index, columns = self.nodes, dtype = np.int32).fillna(0)\n",
    "        for pos, p in enumerate(data['nodes']):\n",
    "            index = self.index[pos]\n",
    "            for lymph_node in p:\n",
    "                monograms.loc[index, lymph_node] = 1\n",
    "        return monograms\n",
    "\n",
    "    def get_bigram_names(self):\n",
    "        bigram_set = set([])\n",
    "\n",
    "        for i, name in enumerate(self.node_list):\n",
    "            for i2 in range(i+1, len(self.node_list)):\n",
    "                if self.adjacency.iloc[i,i2] > 0:\n",
    "                    bigram_set.add(name + self.node_list[i2])\n",
    "        ' '.join(sorted(bigram_set))\n",
    "        bigram_names = (sorted(bigram_set))\n",
    "        return bigram_names, bigram_set\n",
    "\n",
    "    def bigramize(self, v, side):\n",
    "        #shoudl take a unilateral (left or right) matrix of affected lypmh nnodes\n",
    "        assert(v.shape[1] == self.adjacency.shape[1])\n",
    "        col_names = list(v.columns)\n",
    "        clean = lambda x:  re.sub('^[LR]\\s*','', x)\n",
    "        bigrams = []\n",
    "        names = []\n",
    "        _, bigram_set = self.get_bigram_names()\n",
    "        for i, colname in enumerate(col_names):\n",
    "            nodename = clean(colname)\n",
    "            for i2 in range(i+1, v.shape[1]):\n",
    "                colname2 = col_names[i2]\n",
    "                bigram_name = nodename + clean(colname2)\n",
    "                if bigram_name in bigram_set:\n",
    "                    if bigram_name not in names:\n",
    "                        names.append(side + bigram_name)\n",
    "                    bigram_vector = v[colname].values * v[colname2].values\n",
    "                    bigrams.append(bigram_vector.reshape(-1,1))\n",
    "        return pd.DataFrame(np.hstack(bigrams), columns = names, index = self.data.index)\n",
    "    \n",
    "    def formatted_features(self):\n",
    "        datacopy = self.data.copy()\n",
    "        datacopy.columns = [LNDataset.js_name_dict.get(x, x) for x in datacopy.columns]\n",
    "        datacopy.nodes = rename_nodes(datacopy.nodes)\n",
    "        return datacopy\n",
    "    \n",
    "def rename_nodes(nodes):\n",
    "    name_dict = {'RRPLN':'RRN', 'LRPLN':'LRN'}\n",
    "    rename_node = lambda nodelist: [name_dict.get(x,x) for x in nodelist]\n",
    "    return nodes.apply(lambda x: rename_node(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1(x1, x2):\n",
    "    return np.sum(np.abs(x1-x2))\n",
    "\n",
    "def tanimoto_dist(x1, x2):\n",
    "    if l1(x1, x2) == 0:\n",
    "        return 0\n",
    "    tanimoto = x1.dot(x2)/(x1.dot(x1) + x2.dot(x2) - x1.dot(x2))\n",
    "    #guadalupe used 1 - similarity for her clustering\n",
    "    return 1 - tanimoto\n",
    "\n",
    "def l2(x1, x2):\n",
    "    return np.sqrt(np.sum((x1-x2)**2))\n",
    "\n",
    "def dist_matrix(x, dist_func, scale = False):\n",
    "    n = x.shape[0]\n",
    "    distance = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            distance[i,j] = (dist_func(x[i], x[j]))\n",
    "    distance += distance.transpose()\n",
    "    if scale:\n",
    "        distance = (distance - distance.min())/(distance.max() - distance.min())\n",
    "    return distance\n",
    "\n",
    "def sim_matrix(x, dist_func, zero_axis = True):\n",
    "    dist = dist_matrix(x, dist_func, scale = True)\n",
    "    sim = 1 - dist\n",
    "    if zero_axis:\n",
    "        sim[np.arange(x.shape[0]), np.arange(x.shape[0])] = 0\n",
    "    return sim\n",
    "\n",
    "def LN_similarity_result_matrix(ln_dataset):\n",
    "    x = ln_dataset.spatial().values\n",
    "    ids = ln_dataset.index.values.ravel()\n",
    "    similarity = sim_matrix(x, dist_func = tanimoto_dist)\n",
    "    most_similar_ids = []\n",
    "    similarity_scores = []\n",
    "    for pos, sim_row in enumerate(similarity):\n",
    "        sorted_args = np.argsort(-sim_row).ravel()\n",
    "        sorted_sims = sim_row[sorted_args]\n",
    "        sorted_ids = ids[sorted_args]\n",
    "        nonzero = np.argwhere(sorted_sims > 0).ravel()\n",
    "        sorted_sim = np.insert(sorted_sims[nonzero], 0, 1)\n",
    "        sorted_ids = np.insert(sorted_ids[nonzero], 0, ids[pos])\n",
    "        most_similar_ids.append(list(sorted_ids))\n",
    "        similarity_scores.append(list(sorted_sim))\n",
    "    db = pd.DataFrame({'similarity':most_similar_ids, 'scores':similarity_scores},\n",
    "                     index = ln_dataset.index)\n",
    "    return(db)\n",
    "    \n",
    "class FClusterer(ClusterMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self, n_clusters, dist_func = tanimoto_dist, link = 'weighted', criterion = 'maxclust'):\n",
    "        self.link = link\n",
    "        self.dist_func = dist_func if link not in ['median', 'ward', 'centroid'] else 'euclidean'\n",
    "        self.t = n_clusters\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def fit_predict(self, x, y = None):\n",
    "        clusters = linkage(x, method = self.link, metric = self.dist_func)\n",
    "        return fcluster(clusters, self.t, criterion = self.criterion)\n",
    "    \n",
    "def randomized_cohort_data(df, frac = .6):\n",
    "    if isinstance(df, LNDataset):\n",
    "        df = df.data\n",
    "    newdata = df.copy()\n",
    "    for col in newdata.columns:\n",
    "        newdata[col] = newdata[col].sample(frac=1, replace=True).values\n",
    "    newdata.index = np.arange(newdata.shape[0]) + 1\n",
    "    newdata.index.rename('Dummy ID',inplace=True)\n",
    "    newdata = newdata.sample(frac=frac,replace=False)\n",
    "    return newdata.dropna()\n",
    "    \n",
    "def dataset_to_json(dataset, path = 'example_dataset.json'):\n",
    "    #todo: get scores\n",
    "    data = dataset.formatted_features()\n",
    "    similarity_data = LN_similarity_result_matrix(dataset)\n",
    "    data = pd.concat([data, similarity_data],axis=1)\n",
    "    data['id'] = data.index.values.astype('int32')\n",
    "    data.to_json(path, orient='records')\n",
    "    \n",
    "def gen_random_cohort(patient_path='./Anonymized_644.Updated_cleaned_v1.3.2.tsv', \n",
    "                      target_csv='example_patients.csv',\n",
    "                     target_json='example_patients.json',\n",
    "                     target_bigram_cluster_csv = None,\n",
    "                     target_unigram_cluster_csv = None):\n",
    "    dataset = LNDataset(patient_path)\n",
    "    dataset.data = randomized_cohort_data(dataset.data)\n",
    "    dataset = LNDataset(dataset.data)\n",
    "    if target_csv is not None:\n",
    "        try:\n",
    "            dataset.data.drop(['nodes'],axis=1).to_csv(target_csv)\n",
    "            print('saved synthetic data csv to', target_csv)\n",
    "        except:\n",
    "            print('error saving synthetic data as csv')\n",
    "    dataset_to_json(dataset, target_json)\n",
    "    return dataset\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved synthetic data csv to example_patients.csv\n"
     ]
    }
   ],
   "source": [
    "sd = gen_random_cohort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "fclust = FClusterer(6)\n",
    "gclust = FClusterer(int(sd.data.shape[0]/2))\n",
    "bigrams = sd.spatial().values\n",
    "unigrams = sd.dual_monograms.values\n",
    "bigram_clusters = fclust.fit_predict(bigrams)\n",
    "bigram_groups = gclust.fit_predict(bigrams)\n",
    "unigram_clusters = fclust.fit_predict(unigrams)\n",
    "unigram_groups = gclust.fit_predict(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(bigram_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
